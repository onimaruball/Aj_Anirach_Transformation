{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca6e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\ASUS\\Downloads\\all_data_filtered_ver2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e3c8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function all\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.tokenize import DEFAULT_DICT_TRIE as trie\n",
    "import docx2txt\n",
    "token_word = docx2txt.process(r'C:\\Users\\ASUS\\Downloads\\คำสำคัญ-10เอกสาร.docx')\n",
    "token_list  = token_word.split(sep = '\\n\\n') \n",
    "trie.add('ประเวศ วะสี')\n",
    "trie.add('สามเหลี่ยมเขยื้อนภูเขา')\n",
    "trie.add('โควิด')\n",
    "for i in token_list:\n",
    "    trie.add(i)\n",
    "import os\n",
    "import pythainlp.tokenize.crfcut as sentences_cut\n",
    "from pythainlp import word_tokenize\n",
    "import re\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "from pythainlp.tag import pos_tag\n",
    "def create_sentences(dict_sent):\n",
    "    dict_sentname = {}\n",
    "    for i in range(len(dict_sent)):\n",
    "        dict_sentname[i+1] = dict_sent[i]['sentence']\n",
    "    dict_pos = {}\n",
    "    for i in range(len(dict_sent)):\n",
    "        dict_pos[i+1] = dict_sent[i]['token_pos']     \n",
    "    Sentences = [dict_sentname,dict_pos]\n",
    "    dict_all = {}\n",
    "    dict_all['sentence_segmentation'] = dict_sentname\n",
    "    dict_all['token_pos'] = dict_pos\n",
    "    return(dict_all)\n",
    "from pythainlp import word_tokenize\n",
    "import re\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "from pythainlp.tag import pos_tag\n",
    "def list_files(filepath, filetype):\n",
    "    paths = []\n",
    "    filess = []\n",
    "    doc_content = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(filetype.lower()):\n",
    "                paths.append(os.path.join(root, file))\n",
    "                filess.append(file)\n",
    "    for file in paths:\n",
    "        doc_content.append(docx2txt.process(file))\n",
    "    f_df = pd.DataFrame(filess)\n",
    "    d_df = pd.DataFrame(doc_content)\n",
    "    all_df = pd.concat([f_df, d_df], axis = 1)\n",
    "    all_df.columns = ['file name','content']\n",
    "    all_df['token + POS'] = all_df['content'].apply(pipeline_pos)\n",
    "    return(all_df)\n",
    "def pipeline_pos(data):\n",
    "        data = data.replace('\\n','')\n",
    "        data = data.replace('\\t','')\n",
    "        data = data.replace('_','')\n",
    "        data = data.replace('“','')\n",
    "        data = data.replace('”','')\n",
    "        word_tokens = []\n",
    "        pos_tag_word = []\n",
    "        list_sentense = sentences_cut.segment(data)\n",
    "        for i in range(len(list_sentense)):\n",
    "            word_tokens.append(word_tokenize(list_sentense[i]))\n",
    "        for i in range(len(word_tokens)):\n",
    "            pos_tag_word.append(pos_tag(word_tokens[i]))\n",
    "        return(pos_tag_word)\n",
    "def clean_df(data):\n",
    "    data = data.replace('\\n','')\n",
    "    data = data.replace('\\t','')\n",
    "    data = data.replace('_','')\n",
    "    data = data.replace('“','')\n",
    "    data = data.replace('”','')\n",
    "    return(data)\n",
    "def to_dict(row):\n",
    "    return {'originalSentence': row['originalSentence'],\n",
    "            'wordPOS': row['wordPOS']}\n",
    "def chage_llt_to_lll(data):\n",
    "    return(list(map(lambda tuple_: [list(t) for t in tuple_], data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f3712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anirach_transform(data_content,output_name):\n",
    "    test_new = data_content\n",
    "    test_new_clean = clean_df(test_new)\n",
    "    test_sent = sentences_cut.segment(test_new_clean)\n",
    "    word_tokens = []\n",
    "    pos_tag_word = []\n",
    "    for i in range(len(test_sent)):\n",
    "            word_tokens.append(word_tokenize(test_sent[i]))\n",
    "    for i in range(len(word_tokens)):\n",
    "            pos_tag_word.append(pos_tag(word_tokens[i]))\n",
    "    final_json = {'originalSentence':[],'wordPOS':[]}\n",
    "    test_double_list = list(map(lambda tuple_: [list(t) for t in tuple_], pos_tag_word))\n",
    "    for i in range(len(test_sent)):\n",
    "        final_json['originalSentence'].append(test_sent[i])\n",
    "        final_json['wordPOS'].append(test_double_list[i])\n",
    "    df = pd.DataFrame(final_json)\n",
    "    result = df.apply(to_dict, axis=1).tolist()\n",
    "    import json\n",
    "    with open(output_name, \"w\", encoding='utf-8') as outfile:\n",
    "        json.dump(result, outfile,ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e5d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ผนึกกำลังเพื่อความปลอดภัยและสุขภาวะ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20db180",
   "metadata": {},
   "outputs": [],
   "source": [
    "anirach_transform(data.iloc[2].content,'test_again_2.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
